# -*- coding: utf-8 -*-
"""Untitled19.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19osShULhN0XT-7I_PaZD4O6YMVi1n0Wr
"""

# -*- coding: utf-8 -*-
"""Untitled17.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12vydERr9s_zUHKSbfUHDCucWjTDmlUVx
"""

import numpy as np
import cv2
from PIL import Image
import IPython.display as display
import matplotlib.pyplot as plt
from IPython.display import display
import numpy as np
from IPython.display import display as ipydisplay


import os
# 图像分割函数
def image_segmentation(image_data, threshold):
    # Convert the image data to a numpy array with float32 data type
    image_data = np.array(image_data, dtype=np.float32)

    # Use threshold segmentation method to set pixel values greater than threshold to 255 and others to 0
    ret, segmented_image = cv2.threshold(image_data, threshold, 255, cv2.THRESH_BINARY)

    return segmented_image

def remove_noise(image_path, kernel_size=(2, 2), iterations=1):
    # 读取图像
    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)

    # 图像腐蚀操作
    kernel = np.ones(kernel_size, np.uint8)
    eroded = cv2.erode(img, kernel, iterations=iterations)

    # 图像膨胀操作
    dilated = cv2.dilate(eroded, kernel, iterations=iterations)

    # 阈值化处理
    _, thresholded = cv2.threshold(dilated, 1, 255, cv2.THRESH_BINARY)

    return thresholded

def smooth_fiber_distribution(skeleton_image_path, region_image_path, output_dir):
    # Read skeleton line images and convert to binary masks
    skeleton_img = cv2.imread(skeleton_image_path, cv2.IMREAD_GRAYSCALE)
    _, skeleton_mask = cv2.threshold(skeleton_img, 1, 255, cv2.THRESH_BINARY)

    # Read area images
    region_img = cv2.imread(region_image_path, cv2.IMREAD_GRAYSCALE)

    # Applying Morphological Expansion to Skeleton Masks
    kernel = np.ones((3, 3), np.uint8)
    dilated_skeleton_mask = cv2.dilate(skeleton_mask, kernel, iterations=2)

    # Smoothing regional data using an expanded skeleton mask
    smoothed_region_img = cv2.bitwise_and(region_img, dilated_skeleton_mask)

    # Displays the original image and the smoothed image
    plt.figure(figsize=(10, 5))
    plt.subplot(1, 3, 1)
    plt.imshow(skeleton_img, cmap='gray')
    plt.title('骨架线图像')
    plt.axis('off')

    plt.subplot(1, 3, 2)
    plt.imshow(region_img, cmap='gray')
    plt.title('原始纤维分布区域图像')
    plt.axis('off')

    plt.subplot(1, 3, 3)
    plt.imshow(smoothed_region_img, cmap='gray')
    plt.title('平滑后的纤维分布区域图像')
    plt.axis('off')

    # 保存平滑后的纤维分布区域图像
    output_path = os.path.join(output_dir, 'smoothed_region_image.png')
    cv2.imwrite(output_path, smoothed_region_img)

    plt.show()















if __name__ == "__main__":
    # Define the number of images, n (modify this according to your actual case)
    n = 10

    for i in range(1, n+1):
        # Load grayscale image data
        image_path = f"/content/plot_finished_a_{i}.png"
        image_data = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)

        # 设定阈值
        threshold_value = 254

        # 调用图像分割函数，得到分割结果
        segmented_image = image_segmentation(image_data, threshold_value)

        # 显示原始图像和分割后的图像
        plt.figure(figsize=(10, 5))

        plt.subplot(1, 2, 1)
        plt.imshow(image_data, cmap='gray')
        plt.title('Original Image')
        plt.axis('off')

        plt.subplot(1, 2, 2)
        plt.imshow(segmented_image, cmap='gray')
        plt.title('Segmented Image')
        plt.axis('off')

        # 保存分割后的图像
        save_path = f"/content/plot_finished_a_{i}_segmented.png"
        cv2.imwrite(save_path, segmented_image)

        plt.show()

        # 示例用法
        input_image_path = f'/content/plot_finished_a_{i}_segmented.png'
        output_image = remove_noise(input_image_path)

        # 保存输出图像
        output_image_path = f'/content/output_image_{i}.png'
        cv2.imwrite(output_image_path, output_image)

        # 显示输出图像
        output_image = Image.open(output_image_path)
        ipydisplay(output_image)

        # 读取图像
        image_path = f"/content/output_image_{i}.png"
        img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)

        # 二值化图像
        _, thresholded_img = cv2.threshold(img, 1, 255, cv2.THRESH_BINARY)

        # 骨架化
        thinning_img = cv2.ximgproc.thinning(thresholded_img)

        # 保存骨架化图像
        cv2.imwrite(f"/content/skeletonized_image_{i}.png", thinning_img)

        # 绘制中心线在彩色图像上
        color_img = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)
        contours, _ = cv2.findContours(thinning_img, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        for contour in contours:
            for j in range(1, len(contour)):
                x1, y1 = contour[j-1][0]
                x2, y2 = contour[j][0]
                cv2.line(color_img, (x1, y1), (x2, y2), (0, 0, 255), 1)  # 将线宽设置为1

        # 保存彩色图像
        cv2.imwrite(f"/content/centerline_image_{i}.png", color_img)

        # 显示结果
        plt.subplot(1, 2, 1)
        plt.imshow(thresholded_img, cmap='gray')
        plt.title(f"Binary Image {i}")
        plt.axis('off')

        plt.subplot(1, 2, 2)
        plt.imshow(color_img)
        plt.title(f"Centerline Image {i}")
        plt.axis('off')

        plt.show()

        # 使用提供的路径进行平滑操作并保存输出图像
        skeleton_image_path = f'/content/skeletonized_image_{i}.png'
        region_image_path = f'/content/plot_finished_a_{i}.png'
        output_dir = '/content/output'  # Replace with the desired output directory

        # 确保输出目录存在
        os.makedirs(output_dir, exist_ok=True)

        smooth_fiber_distribution(skeleton_image_path, region_image_path, output_dir)







import numpy as np
import cv2
from PIL import Image
import IPython.display as display
import matplotlib.pyplot as plt
from IPython.display import display as ipydisplay

import os

def image_segmentation(image_data, threshold):
    if image_data is None or not isinstance(image_data, np.ndarray):
        raise ValueError("Invalid input. image_data must be a valid numpy array.")

    if len(image_data.shape) == 3:
        image_data = cv2.cvtColor(image_data, cv2.COLOR_BGR2GRAY)

    ret, segmented_image = cv2.threshold(image_data, threshold, 255, cv2.THRESH_BINARY)

    return segmented_image

def remove_noise(image_path, kernel_size=(2, 2), iterations=1):
    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)

    kernel = np.ones(kernel_size, np.uint8)
    eroded = cv2.erode(img, kernel, iterations=iterations)

    dilated = cv2.dilate(eroded, kernel, iterations=iterations)

    _, thresholded = cv2.threshold(dilated, 1, 255, cv2.THRESH_BINARY)

    return thresholded

def smooth_fiber_distribution(skeleton_image_path, region_image_path, output_dir):
    skeleton_img = cv2.imread(skeleton_image_path, cv2.IMREAD_GRAYSCALE)
    _, skeleton_mask = cv2.threshold(skeleton_img, 1, 255, cv2.THRESH_BINARY)

    region_img = cv2.imread(region_image_path, cv2.IMREAD_GRAYSCALE)

    kernel = np.ones((3, 3), np.uint8)
    dilated_skeleton_mask = cv2.dilate(skeleton_mask, kernel, iterations=2)

    smoothed_region_img = cv2.bitwise_and(region_img, dilated_skeleton_mask)

    plt.figure(figsize=(10, 5))
    plt.subplot(1, 3, 1)
    plt.imshow(skeleton_img, cmap='gray')
    plt.title('骨架线图像')
    plt.axis('off')

    plt.subplot(1, 3, 2)
    plt.imshow(region_img, cmap='gray')
    plt.title('原始纤维分布区域图像')
    plt.axis('off')

    plt.subplot(1, 3, 3)
    plt.imshow(smoothed_region_img, cmap='gray')
    plt.title('平滑后的纤维分布区域图像')
    plt.axis('off')

    output_path = os.path.join(output_dir, 'smoothed_region_image.png')
    cv2.imwrite(output_path, smoothed_region_img)

    plt.show()

if __name__ == "__main__":
    n = 49

    for i in range(1, n+1):
        image_path = f"/content/plot_finished_a_{i}.png"
        image_data = cv2.imread(image_path)

        if image_data is None:
            raise ValueError(f"Unable to load image at path: {image_path}")

        threshold_value = 254

        segmented_image = image_segmentation(image_data, threshold_value)

        plt.figure(figsize=(10, 5))
        plt.subplot(1, 2, 1)
        plt.imshow(image_data, cmap='gray')
        plt.title('Original Image')
        plt.axis('off')

        plt.subplot(1, 2, 2)
        plt.imshow(segmented_image, cmap='gray')
        plt.title('Segmented Image')
        plt.axis('off')

        save_path = f"/content/plot_finished_a_{i}_segmented.png"
        cv2.imwrite(save_path, segmented_image)

        plt.show()

        input_image_path = f'/content/plot_finished_a_{i}_segmented.png'
        output_image = remove_noise(input_image_path)

        output_image_path = f'/content/output_image_{i}.png'
        cv2.imwrite(output_image_path, output_image)

        output_image = Image.open(output_image_path)
        ipydisplay(output_image)

        image_path = f"/content/output_image_{i}.png"
        img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)

        _, thresholded_img = cv2.threshold(img, 1, 255, cv2.THRESH_BINARY)

        thinning_img = cv2.ximgproc.thinning(thresholded_img)

        cv2.imwrite(f"/content/skeletonized_image_{i}.png", thinning_img)

        color_img = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)
        contours, _ = cv2.findContours(thinning_img, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        for contour in contours:
            for j in range(1, len(contour)):
                x1, y1 = contour[j-1][0]
                x2, y2 = contour[j][0]
                cv2.line(color_img, (x1, y1), (x2, y2), (0, 0, 255), 1)

        cv2.imwrite(f"/content/centerline_image_{i}.png", color_img)

        plt.subplot(1, 2, 1)
        plt.imshow(thresholded_img, cmap='gray')
        plt.title(f"Binary Image {i}")
        plt.axis('off')

        plt.subplot(1, 2, 2)
        plt.imshow(color_img)
        plt.title(f"Centerline Image {i}")
        plt.axis('off')

        plt.show()

        skeleton_image_path = f'/content/skeletonized_image_{i}.png'
        region_image_path = f'/content/plot_finished_a_{i}.png'
        output_dir = '/content/output'

        os.makedirs(output_dir, exist_ok=True)

        smooth_fiber_distribution(skeleton_image_path, region_image_path, output_dir)



import os
import cv2
import numpy as np

def merge_images(directory):
    # 获取目录中所有`skeletonized_image_n.png`文件的列表
    image_files = [file for file in os.listdir(directory) if file.startswith('skeletonized_image_')]

    if not image_files:
        print("目录中没有找到`skeletonized_image_n.png`文件。")
        return

    # 读取第一张图片以获取其维度
    first_image_path = os.path.join(directory, image_files[0])
    first_image = cv2.imread(first_image_path, cv2.IMREAD_GRAYSCALE)
    merged_image = np.zeros_like(first_image, dtype=np.uint8)

    for image_file in image_files:
        image_path = os.path.join(directory, image_file)
        img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)

        # 在整合图片和当前图片之间找到最大值
        merged_image = np.maximum(merged_image, img)

    return merged_image

if __name__ == "__main__":
    directory = "/content"  # 将此处替换为包含skeletonized images的目录

    merged_image = merge_images(directory)

    # 显示整合后的图片
    plt.imshow(merged_image, cmap='gray')
    plt.title("整合后的图片")
    plt.axis('off')
    plt.show()

    # 保存整合后的图片
    cv2.imwrite("/content/merged_image.png", merged_image)

import cv2
import numpy as np
from sklearn.cluster import DBSCAN
import matplotlib.pyplot as plt

# Load grayscale image
image_path = "/content/merged_image.png"
gray_image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)

# Extract non-zero pixel coordinates and values
non_zero_indices = np.nonzero(gray_image)
y, x = non_zero_indices[0], non_zero_indices[1]
pixel_values = gray_image[y, x]

# Merge coordinates (x, y) and pixel values (pixel_values) into a feature matrix (X)
X = np.column_stack((x, y))

# Perform DBSCAN clustering
# Epsilon (eps) and MinPts are the key parameters of DBSCAN
# Adjust their values based on your data and clustering requirements
eps = 5  # Adjust this value
min_pts = 5  # Adjust this value
dbscan = DBSCAN(eps=eps, min_samples=min_pts)
dbscan_labels = dbscan.fit_predict(X)

# Visualize the grayscale image and clustering results
plt.figure(figsize=(12, 6))

# Plot the grayscale image
plt.subplot(1, 2, 1)
plt.imshow(gray_image, cmap='gray')
plt.title("Grayscale Image")

# Plot the clustering results
plt.subplot(1, 2, 2)
plt.scatter(x, y, c=dbscan_labels, cmap='viridis', marker='o', s=50)
plt.title("DBSCAN Clustering Result")
plt.xlabel("X-coordinate")
plt.ylabel("Y-coordinate")

plt.show()

import cv2
import numpy as np
from sklearn.cluster import DBSCAN
import matplotlib.pyplot as plt

# 加载灰度图像
image_path = "/content/merged_image.png"
gray_image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)

# 提取非零像素点的坐标和像素值
non_zero_indices = np.nonzero(gray_image)
y, x = non_zero_indices[0], non_zero_indices[1]
pixel_values = gray_image[y, x]
print(pixel_values)
# 将坐标（x, y）和像素值（pixel_values）合并成一个特征矩阵（X），其中（x, y）是两列坐标
X = np.column_stack((x, y))

# Perform DBSCAN clustering
# Epsilon (eps) and MinPts are two key parameters of DBSCAN
# Adjust their values according to your data and clustering requirements
eps = 1  # Adjust this value
min_pts = 1  # Adjust this value
dbscan = DBSCAN(eps=eps, min_samples=min_pts)
dbscan_labels = dbscan.fit_predict(X)

# 可视化聚类结果
plt.figure(figsize=(12, 6))

# 绘制灰度图像
plt.subplot(1, 2, 1)
plt.imshow(gray_image, cmap='gray')
plt.title("灰度图像")

# 绘制聚类结果，并翻转y轴
plt.subplot(1, 2, 2)
plt.scatter(x, y, c=dbscan_labels, cmap='viridis', marker='o', s=5)
plt.title("DBSCAN聚类结果")
plt.xlabel("X坐标")
plt.ylabel("Y坐标")
plt.gca().invert_yaxis()

plt.show()



import cv2
import numpy as np
from sklearn.cluster import DBSCAN
import matplotlib.pyplot as plt

# 加载灰度图像
image_path = "/content/merged_image.png"
gray_image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)

# 提取非零像素点的坐标和像素值
non_zero_indices = np.nonzero(gray_image)
y, x = non_zero_indices[0], non_zero_indices[1]
pixel_values = gray_image[y, x]

# 将坐标（x, y）和像素值（pixel_values）合并成一个特征矩阵（X），其中（x, y）是两列坐标
X = np.column_stack((x, y))

# 进行DBSCAN聚类
# Epsilon（eps）和MinPts是DBSCAN的两个关键参数
# 根据你的数据和聚类要求调整它们的值
eps = 3  # 调整此值
min_pts = 5 # 调整此值
dbscan = DBSCAN(eps=eps, min_samples=min_pts)
dbscan_labels = dbscan.fit_predict(X)

# 计算聚类的数量（排除噪声点）
n_clusters = len(np.unique(dbscan_labels)) - 1

# 输出聚类数量
print("聚类数量:", n_clusters)

# 可视化聚类结果
plt.figure(figsize=(12, 6))

# 绘制灰度图像
plt.subplot(1, 2, 1)
plt.imshow(gray_image, cmap='gray')
plt.title("灰度图像")

# 绘制聚类结果，并翻转y轴
plt.subplot(1, 2, 2)
plt.scatter(x, y, c=dbscan_labels, cmap='viridis', marker='o', s=5)
plt.title("DBSCAN聚类结果")
plt.xlabel("X坐标")
plt.ylabel("Y坐标")
plt.gca().invert_yaxis()

plt.show()

import cv2
import numpy as np
from sklearn.cluster import DBSCAN
import matplotlib.pyplot as plt

# 加载灰度图像
image_path = "/content/merged_image.png"
gray_image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)

# 提取非零像素点的坐标和像素值
non_zero_indices = np.nonzero(gray_image)
y, x = non_zero_indices[0], non_zero_indices[1]
pixel_values = gray_image[y, x]

# 将坐标（x, y）和像素值（pixel_values）合并成一个特征矩阵（X），其中（x, y）是两列坐标
X = np.column_stack((x, y))

# 进行DBSCAN聚类
# Epsilon（eps）和MinPts是DBSCAN的两个关键参数
# 根据你的数据和聚类要求调整它们的值
eps = 3 # 调整此值
min_pts = 2 # 调整此值
dbscan = DBSCAN(eps=eps, min_samples=min_pts)
dbscan_labels = dbscan.fit_predict(X)

# 计算聚类的数量（排除噪声点）
n_clusters = len(np.unique(dbscan_labels)) - 1

# 输出聚类数量
print("聚类数量:", n_clusters)

# 创建字典用于存储每个聚类的坐标点
cluster_points = {}
for i in range(n_clusters):
    cluster_points[f'line_{i}'] = []

# 将点根据聚类结果存入字典
for label, (x_val, y_val) in zip(dbscan_labels, zip(x, y)):
    if label != -1:  # Ignore noise points (label == -1)
        cluster_points[f'line_{label}'].append((x_val, y_val))

# 可视化聚类结果
plt.figure(figsize=(12, 6))

# 绘制灰度图像
plt.subplot(1, 2, 1)
plt.imshow(gray_image, cmap='gray')
plt.title("灰度图像")

# 绘制聚类结果，并翻转y轴
plt.subplot(1, 2, 2)
for i in range(n_clusters):
    points = np.array(cluster_points[f'line_{i}'])
    plt.scatter(points[:, 0], points[:, 1], cmap='viridis', marker='o', s=5, label=f'line_{i}')
plt.title("DBSCAN聚类结果")
plt.xlabel("X坐标")
plt.ylabel("Y坐标")
plt.gca().invert_yaxis()
plt.legend()
plt.show()

# 将聚类结果保存为数组
for i in range(n_clusters):
    np.savetxt(f'cluster_{i}.txt', np.array(cluster_points[f'line_{i}']), fmt='%d')



import cv2
import numpy as np
from sklearn.cluster import DBSCAN
import matplotlib.pyplot as plt

# 加载灰度图像
image_path = "/content/merged_image.png"
gray_image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)

# 提取非零像素点的坐标和像素值
non_zero_indices = np.nonzero(gray_image)
y, x = non_zero_indices[0], non_zero_indices[1]
pixel_values = gray_image[y, x]

# 将坐标（x, y）和像素值（pixel_values）合并成一个特征矩阵（X），其中（x, y）是两列坐标
X = np.column_stack((x, y))

# 进行DBSCAN聚类
# Epsilon（eps）和MinPts是DBSCAN的两个关键参数
# 根据你的数据和聚类要求调整它们的值
eps = 3 # 调整此值
min_pts = 3 # 调整此值
dbscan = DBSCAN(eps=eps, min_samples=min_pts)
dbscan_labels = dbscan.fit_predict(X)

# 计算聚类的数量（排除噪声点）
n_clusters = len(np.unique(dbscan_labels)) - 1

# 输出聚类数量
print("聚类数量:", n_clusters)

# 创建字典用于存储每个聚类的坐标点
cluster_points = {}
for i in range(n_clusters):
    cluster_points[f'line_{i}'] = []

# 将点根据聚类结果存入字典
for label, (x_val, y_val) in zip(dbscan_labels, zip(x, y)):
    if label != -1:  # Ignore noise points (label == -1)
        cluster_points[f'line_{label}'].append((x_val, y_val))

# 可视化聚类结果
plt.figure(figsize=(12, 6))

# 绘制灰度图像
plt.subplot(1, 2, 1)
plt.imshow(gray_image, cmap='gray')
plt.title("灰度图像")

# 绘制聚类结果，并翻转y轴
plt.subplot(1, 2, 2)
for i in range(n_clusters):
    points = np.array(cluster_points[f'line_{i}'])
    plt.scatter(points[:, 0], points[:, 1], cmap='viridis', marker='o', s=5, label=f'line_{i}')
plt.title("DBSCAN聚类结果")
plt.xlabel("X坐标")
plt.ylabel("Y坐标")
plt.gca().invert_yaxis()
plt.legend()
plt.show()

# 将聚类结果保存为数组
for i in range(n_clusters):
    cluster_points_array = np.array(cluster_points[f'line_{i}'])
    np.savetxt(f'cluster_{i}.txt', cluster_points_array, fmt='%d', delimiter='\t')
    print(f"聚类{i}的坐标已保存至cluster_{i}.txt文件")

import cv2
import numpy as np

# 加载灰度图像
image_path = "/content/merged_image.png"
gray_image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)

# 提取非黑色像素点的坐标
non_zero_indices = np.nonzero(gray_image)
y, x = non_zero_indices[0], non_zero_indices[1]

# 打印非黑色像素点的坐标
print("非黑色像素点的坐标：")
for i in range(len(x)):
    print(f"({x[i]}, {y[i]})")

# 计算非黑色像素点的数量
num_non_black_pixels = len(x)
print("非黑色像素点的数量：", num_non_black_pixels)

import cv2
import numpy as np

# 加载灰度图像
image_path = "/content/merged_image.png"
gray_image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)

# 提取非黑色像素点的坐标
non_zero_indices = np.nonzero(gray_image)
y, x = non_zero_indices[0], non_zero_indices[1]

# 打印非黑色像素点的坐标
print("非黑色像素点的坐标：")
for i in range(len(x)):
    print(f"({x[i]}, {y[i]})")

# 计算非黑色像素点的数量
num_non_black_pixels = len(x)
print("非黑色像素点的数量：", num_non_black_pixels)

# 保存输出为.txt文件
output_file = "non_black_pixels.txt"
with open(output_file, 'w') as file:
    file.write("非黑色像素点的坐标：\n")
    for i in range(len(x)):
        file.write(f"({x[i]}, {y[i]})\n")
    file.write("非黑色像素点的数量：" + str(num_non_black_pixels) + "\n")

print("输出已保存到", output_file)

# 读取文件内容
with open("/content/non_black_pixels.txt", 'r') as file:
    content = file.read()

# 去除括号，只保留数字，并保存到新文件
data = []
for line in content.split("\n"):
    line = line.strip()
    if line.startswith("(") and line.endswith(")"):
        line = line[1:-1]  # 去除括号
        x, y = map(int, line.split(","))
        data.append((x, y))

# 输出处理后的数据
print("处理后的数据：")
for point in data:
    print(f"({point[0]}, {point[1]})")

# 保存处理后的数据到新文件，去除数字之间的逗号
output_file = "/content/processed_non_black_pixels.txt"
with open(output_file, 'w') as file:
    for point in data:
        file.write(f"{point[0]} {point[1]}\n")

print("处理后的数据已保存到", output_file)



import os
import shutil

def count_coordinates(file_path):
    # 读取文件并统计坐标点数量
    with open(file_path, 'r') as file:
        lines = file.readlines()
    return len(lines)

def main():
    # 创建目标文件夹
    cluster_lower_dir = "/content/cluster_lower/"
    cluster_high_dir = "/content/cluster_high/"
    os.makedirs(cluster_lower_dir, exist_ok=True)
    os.makedirs(cluster_high_dir, exist_ok=True)

    # 遍历/content/目录下的所有文件
    for filename in os.listdir("/content/"):
        if filename.startswith("cluster_") and filename.endswith(".txt"):
            file_path = os.path.join("/content/", filename)
            coordinates_count = count_coordinates(file_path)
            if coordinates_count < 10:
                # 将文件复制到/content/cluster_lower/下
                shutil.copy(file_path, cluster_lower_dir)
            else:
                # 将文件复制到/content/cluster_high/下
                shutil.copy(file_path, cluster_high_dir)

if __name__ == "__main__":
    main()

import os

def rename_files_in_cluster_high():
    cluster_high_dir = "/content/cluster_high/"
    file_list = os.listdir(cluster_high_dir)
    i = 1
    for filename in file_list:
        if filename.endswith(".txt"):
            new_filename = f"cluster_{i}.txt"
            os.rename(os.path.join(cluster_high_dir, filename), os.path.join(cluster_high_dir, new_filename))
            i += 1

if __name__ == "__main__":
    rename_files_in_cluster_high()

import os
import zipfile

def zip_folder(source_folder, output_filename):
    with zipfile.ZipFile(output_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:
        for root, _, files in os.walk(source_folder):
            for file in files:
                file_path = os.path.join(root, file)
                arcname = os.path.relpath(file_path, source_folder)
                zipf.write(file_path, arcname)

if __name__ == "__main__":
    source_folder = "/content/cluster_high/"
    output_filename = "/content/cluster_high.zip"
    zip_folder(source_folder, output_filename)
    print(f"{source_folder} 中的内容已压缩到 {output_filename}")











import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import SpectralClustering

# 从文件读取数据
file_path = '/content/cluster_6.txt'
data = np.genfromtxt(file_path, delimiter=' ', skip_header=1)

# 如果数据是1D数组，则转换为2D数组
if data.ndim == 1:
    data = data.reshape(-1, 1)

# 对数据按照x轴值进行排序
data = data[data[:, 0].argsort()]

# 谱聚类
n_clusters = 1  # 假设你希望聚类成3个S型曲线
n_neighbors = min(n_clusters + 1, len(data) - 1)  # 设置n_neighbors小于等于数据样本数量
spectral_clustering = SpectralClustering(n_clusters=n_clusters, affinity='nearest_neighbors', n_neighbors=n_neighbors)
labels = spectral_clustering.fit_predict(data)

# 提取簇的数据
clustered_data = []
for i in range(n_clusters):
    clustered_data.append(data[labels == i])

# 曲线拟合函数，使用三次多项式拟合
def fit_s_curve(x, y):
    z = np.polyfit(x, y, 3)  # 使用三次多项式进行拟合
    p = np.poly1d(z)
    return p

# 绘制拟合曲线
plt.figure(figsize=(10, 5))
for cluster in clustered_data:
    x = cluster[:, 0]
    y = cluster[:, 1]
    fit_curve = fit_s_curve(x, y)
    x_new = np.linspace(min(x), max(x), 100)
    y_new = fit_curve(x_new)
    plt.plot(x_new, y_new, label='S Curve')
plt.scatter(data[:, 0], data[:, 1], c=labels, cmap='viridis', label='Data')
plt.xlabel('X')
plt.ylabel('y')
plt.title('Multiple S Curves Fitting using Spectral Clustering')
plt.legend()
plt.show()





import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import SpectralClustering
from scipy.optimize import curve_fit

def fit_curve(x, y):
    def linear(x, a, b):
        return a * x + b

    def quadratic(x, a, b, c):
        return a * x**2 + b * x + c

    def cubic(x, a, b, c, d):
        return a * x**3 + b * x**2 + c * x + d

    def fourth_degree(x, a, b, c, d, e):
        return a * x**4 + b * x**3 + c * x**2 + d * x + e

    if len(x) < 3:
        # Linear fitting for less than 3 data points
        popt, _ = curve_fit(linear, x, y)
    elif len(x) < 6:
        # Quadratic fitting for 3 to 5 data points
        popt, _ = curve_fit(quadratic, x, y)
    elif len(x) < 9:
        # Cubic fitting for 6 to 8 data points
        popt, _ = curve_fit(cubic, x, y)
    else:
        # Fourth-degree polynomial fitting for more than 8 data points
        popt, _ = curve_fit(fourth_degree, x, y)

    p = np.poly1d(popt)
    return p

# 读取数据
data = np.loadtxt('/content/cluster_10.txt')
x = data[:, 0]
y = data[:, 1]

# 谱聚类
n_clusters = 3  # 假设分为3类，你可以根据具体情况调整
spectral_model = SpectralClustering(n_clusters=n_clusters, affinity='nearest_neighbors', n_neighbors=5)
y_pred = spectral_model.fit_predict(data)

# 绘制聚类结果
for i in range(n_clusters):
    cluster_x = x[y_pred == i]
    cluster_y = y[y_pred == i]
    curve = fit_curve(cluster_x, cluster_y)
    plt.plot(cluster_x, cluster_y, 'o', label=f'Cluster {i+1}')
    plt.plot(cluster_x, curve(cluster_x), label=f'Curve {i+1}')

plt.xlabel('X')
plt.ylabel('Y')
plt.legend()
plt.show()





import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import SpectralClustering

# 从文件读取数据
file_path = '/content/cluster_6.txt'
data = np.genfromtxt(file_path, delimiter=' ', skip_header=1)

# 如果数据是1D数组，则转换为2D数组
if data.ndim == 1:
    data = data.reshape(-1, 1)

# 对数据按照x轴值进行排序
data = data[data[:, 0].argsort()]

# 谱聚类
n_clusters = 1  # 假设你希望聚类成3个S型曲线
n_neighbors = min(n_clusters - 1, len(data) - 1)  # 设置n_neighbors为较小的值
spectral_clustering = SpectralClustering(n_clusters=n_clusters, affinity='nearest_neighbors', n_neighbors=n_neighbors)
labels = spectral_clustering.fit_predict(data)

# 提取簇的数据
clustered_data = []
for i in range(n_clusters):
    clustered_data.append(data[labels == i])

# 曲线拟合函数，使用三次多项式拟合
def fit_s_curve(x, y):
    z = np.polyfit(x, y, 3)  # 使用三次多项式进行拟合
    p = np.poly1d(z)
    return p

# 绘制拟合曲线
plt.figure(figsize=(10, 5))
for cluster in clustered_data:
    x = cluster[:, 0]
    y = cluster[:, 1]
    fit_curve = fit_s_curve(x, y)
    x_new = np.linspace(min(x), max(x), 100)
    y_new = fit_curve(x_new)
    plt.plot(x_new, y_new, label='S Curve')
plt.scatter(data[:, 0], data[:, 1], c=labels, cmap='viridis', label='Data')
plt.xlabel('X')
plt.ylabel('y')
plt.title('Multiple S Curves Fitting using Spectral Clustering')
plt.legend()
plt.show()

import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import SpectralClustering
from scipy.optimize import curve_fit

def fit_curve(x, y):
    def linear(x, a, b):
        return a * x + b

    def quadratic(x, a, b, c):
        return a * x**2 + b * x + c

    def cubic(x, a, b, c, d):
        return a * x**3 + b * x**2 + c * x + d

    def fourth_degree(x, a, b, c, d, e):
        return a * x**4 + b * x**3 + c * x**2 + d * x + e

    if len(x) < 3:
        # Linear fitting for less than 3 data points
        popt, _ = curve_fit(linear, x, y)
    elif len(x) < 6:
        # Quadratic fitting for 3 to 5 data points
        popt, _ = curve_fit(quadratic, x, y)
    elif len(x) < 9:
        # Cubic fitting for 6 to 8 data points
        popt, _ = curve_fit(cubic, x, y)
    else:
        # Fourth-degree polynomial fitting for more than 8 data points
        popt, _ = curve_fit(fourth_degree, x, y)

    p = np.poly1d(popt)
    return p

# 读取数据
data = np.loadtxt('/content/cluster_10.txt')
x = data[:, 0]
y = data[:, 1]

# 谱聚类
n_clusters = 3  # 假设分为3类，你可以根据具体情况调整
n_neighbors = min(5, len(x) - 1)  # 设置n_neighbors为5或样本点数-1中的较小值
spectral_model = SpectralClustering(n_clusters=n_clusters, affinity='nearest_neighbors', n_neighbors=n_neighbors)
y_pred = spectral_model.fit_predict(data)

# 绘制聚类结果
for i in range(n_clusters):
    cluster_x = x[y_pred == i]
    cluster_y = y[y_pred == i]
    curve = fit_curve(cluster_x, cluster_y)
    plt.plot(cluster_x, cluster_y, 'o', label=f'Cluster {i+1}')
    plt.plot(cluster_x, curve(cluster_x), label=f'Curve {i+1}')

plt.xlabel('X')
plt.ylabel('Y')
plt.legend()
plt.show()







import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import SpectralClustering
from scipy.interpolate import CubicSpline
import os

def fit_s_curve(x, y):
    # 使用二次多项式进行拟合
    z = np.polyfit(x, y, 3)
    p = np.poly1d(z)
    return p

def process_data(file_path):
    # 从文件读取数据
    data = np.genfromtxt(file_path, delimiter=' ', skip_header=1)

    # 对数据按照x轴值进行排序
    data = data[data[:, 0].argsort()]

    # 谱聚类
    n_clusters = 1  # 假设你希望聚类成3个S型曲线
    spectral_clustering = SpectralClustering(n_clusters=n_clusters, affinity='rbf')  # 使用'rbf'方式计算亲和矩阵
    labels = spectral_clustering.fit_predict(data)

    # 提取簇的数据
    clustered_data = []
    for i in range(n_clusters):
        clustered_data.append(data[labels == i])

    # 绘制拟合曲线
    plt.figure(figsize=(10, 5))
    for cluster in clustered_data:
        x = cluster[:, 0]
        y = cluster[:, 1]
        fit_curve = fit_s_curve(x, y)
        x_new = np.linspace(min(x), max(x), 100)
        y_new = fit_curve(x_new)
        plt.plot(x_new, y_new, label='S Curve')
    plt.scatter(data[:, 0], data[:, 1], c=labels, cmap='viridis', label='Data')
    plt.xlabel('X')
    plt.ylabel('y')
    plt.title('Multiple S Curves Fitting using Spectral Clustering')
    plt.legend()
    plt.show()

# 批量处理数据
data_folder = '/content'
for file_name in os.listdir(data_folder):
    if file_name.startswith('cluster_') and file_name.endswith('.txt'):
        file_path = os.path.join(data_folder, file_name)
        process_data(file_path)

import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import SpectralClustering
import os

from scipy.optimize import curve_fit

def fit_curve(x, y):
    def linear(x, a, b):
        return a * x + b

    def quadratic(x, a, b, c):
        return a * x**2 + b * x + c

    def cubic(x, a, b, c, d):
        return a * x**3 + b * x**2 + c * x + d

    def fourth_degree(x, a, b, c, d, e):
        return a * x**4 + b * x**3 + c * x**2 + d * x + e

    if len(x) < 3:
        # Linear fitting for less than 3 data points
        popt, _ = curve_fit(linear, x, y)
    elif len(x) < 6:
        # Quadratic fitting for 3 to 5 data points
        popt, _ = curve_fit(quadratic, x, y)
    elif len(x) < 9:
        # Cubic fitting for 6 to 8 data points
        popt, _ = curve_fit(cubic, x, y)
    else:
        # Fourth-degree polynomial fitting for more than 8 data points
        popt, _ = curve_fit(fourth_degree, x, y)

    p = np.poly1d(popt)
    return p



def process_data(file_path):
    # Read data from the file
    data = np.genfromtxt(file_path, delimiter=' ', skip_header=1)

    # Sort data based on the x-axis values
    data = data[data[:, 0].argsort()]

    # Perform spectral clustering only if there are more than 9 data points
    if data.shape[0] > 9:
        n_clusters = 3  # Assume you want to cluster into 3 S-curve segments
        spectral_clustering = SpectralClustering(n_clusters=n_clusters, affinity='rbf')  # Using 'rbf' affinity matrix
        labels = spectral_clustering.fit_predict(data)

        # Extract clustered data
        clustered_data = []
        for i in range(n_clusters):
            clustered_data.append(data[labels == i])

        # Plot the fitted curves
        plt.figure(figsize=(10, 5))
        for cluster in clustered_data:
            x = cluster[:, 0]
            y = cluster[:, 1]
            fitted_curve = fit_curve(x, y)  # Use a different variable name here
            x_new = np.linspace(min(x), max(x), 100)
            y_new = fitted_curve(x_new)
            plt.plot(x_new, y_new, label='S Curve')
        plt.scatter(data[:, 0], data[:, 1], c=labels, cmap='viridis', label='Data')
        plt.xlabel('X')
        plt.ylabel('y')
        plt.title('Multiple S Curves Fitting using Spectral Clustering')
        plt.legend()
        plt.show()
    else:
        # For less than 9 data points, do a single curve fitting
        x = data[:, 0]
        y = data[:, 1]
        fitted_curve = fit_curve(x, y)  # Use a different variable name here

        # Plot the fitted curve
        plt.figure(figsize=(10, 5))
        x_new = np.linspace(min(x), max(x), 100)
        y_new = fitted_curve(x_new)
        plt.plot(x_new, y_new, label='Fitted Curve')
        plt.scatter(data[:, 0], data[:, 1], label='Data')
        plt.xlabel('X')
        plt.ylabel('y')
        plt.title('Curve Fitting for Data')
        plt.legend()
        plt.show()

# Batch process data
data_folder = '/content'  # Change this path to your data folder
for file_name in os.listdir(data_folder):
    if file_name.startswith('cluster_') and file_name.endswith('.txt'):
        file_path = os.path.join(data_folder, file_name)
        process_data(file_path)

def process_data(file_path):
    # Read data from the file
    data = np.genfromtxt(file_path, delimiter=' ', skip_header=1)

    # Sort data based on the x-axis values
    sorted_indices = np.argsort(data[:, 0])
    data = data[sorted_indices]

    # Perform spectral clustering only if there are more than 9 data points
    if data.shape[0] > 9:
        n_clusters = 3  # Assume you want to cluster into 3 S-curve segments
        spectral_clustering = SpectralClustering(n_clusters=n_clusters, affinity='rbf')  # Using 'rbf' affinity matrix
        labels = spectral_clustering.fit_predict(data)

        # Extract clustered data
        clustered_data = []
        for i in range(n_clusters):
            clustered_data.append(data[labels == i])

        # Plot the fitted curves
        plt.figure(figsize=(10, 5))
        for cluster in clustered_data:
            x = cluster[:, 0]
            y = cluster[:, 1]
            fitted_curve = fit_curve(x, y)
            x_new = np.linspace(min(x), max(x), 100)
            y_new = fitted_curve(x_new)
            plt.plot(x_new, y_new, label='S Curve')
        plt.scatter(data[:, 0], data[:, 1], c=labels, cmap='viridis', label='Data')
        plt.xlabel('X')
        plt.ylabel('y')
        plt.title('Multiple S Curves Fitting using Spectral Clustering')
        plt.legend()
        plt.show()
    else:
        # For less than 9 data points, do a single curve fitting
        x = data[:, 0]
        y = data[:, 1]
        fitted_curve = fit_curve(x, y)

        # Plot the fitted curve
        plt.figure(figsize=(10, 5))
        x_new = np.linspace(min(x), max(x), 100)
        y_new = fitted_curve(x_new)
        plt.plot(x_new, y_new, label='Fitted Curve')
        plt.scatter(data[:, 0], data[:, 1], label='Data')
        plt.xlabel('X')
        plt.ylabel('y')
        plt.title('Curve Fitting for Data')
        plt.legend()
        plt.show()

# Batch process data
data_folder = '/content'  # Change this path to your data folder
for file_name in os.listdir(data_folder):
    if file_name.startswith('cluster_') and file_name.endswith('.txt'):
        file_path = os.path.join(data_folder, file_name)
        process_data(file_path)







import cv2
import numpy as np
from sklearn.cluster import DBSCAN
import matplotlib.pyplot as plt

# 加载灰度图像
image_path = "/content/merged_image.png"
gray_image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)

# 提取非零像素点的坐标和像素值
non_zero_indices = np.nonzero(gray_image)
y, x = non_zero_indices[0], non_zero_indices[1]
pixel_values = gray_image[y, x]

# 将坐标（x, y）和像素值（pixel_values）合并成一个特征矩阵（X），其中（x, y）是两列坐标
X = np.column_stack((x, y))

# 进行DBSCAN聚类
# Epsilon（eps）和MinPts是DBSCAN的两个关键参数
# 根据你的数据和聚类要求调整它们的值
eps = 3  # 调整此值
min_pts = 4  # 调整此值
dbscan = DBSCAN(eps=eps, min_samples=min_pts)
dbscan_labels = dbscan.fit_predict(X)

# 计算聚类的数量（排除噪声点）
n_clusters = len(np.unique(dbscan_labels)) - 1

# 输出聚类数量
print("聚类数量:", n_clusters)

# 可视化聚类结果
plt.figure(figsize=(12, 6))

# 绘制灰度图像
plt.subplot(1, 2, 1)
plt.imshow(gray_image, cmap='gray')
plt.title("灰度图像")

# 绘制聚类结果，并翻转y轴
plt.subplot(1, 2, 2)
plt.scatter(x, y, c=dbscan_labels, cmap='viridis', marker='o', s=5)
plt.title("DBSCAN聚类结果")
plt.xlabel("X坐标")
plt.ylabel("Y坐标")
plt.gca().invert_yaxis()

plt.show()

# 循环遍历每个聚类并在坐标系上显示数据
for cluster_id in np.unique(dbscan_labels):
    # 跳过噪声点（cluster_id = -1）
    if cluster_id == -1:
        continue

    # 提取属于当前聚类的数据点
    cluster_points = X[dbscan_labels == cluster_id]

    # 创建一个空白图像
    cluster_image = np.zeros_like(gray_image)

    # 在空白图像上用聚类数据点的像素值填充
    for point in cluster_points:
        x, y = point
        cluster_image[y, x] = gray_image[y, x]

    # 可视化单独的聚类图像
    plt.figure(figsize=(5, 5))
    plt.imshow(cluster_image, cmap='gray')
    plt.title(f"聚类 {cluster_id}")
    plt.axis('off')
    plt.show()

import cv2
import numpy as np
from sklearn.cluster import DBSCAN
import matplotlib.pyplot as plt

# 加载灰度图像
image_path = "/content/merged_image.png"
gray_image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)

# 提取非零像素点的坐标和像素值
non_zero_indices = np.nonzero(gray_image)
y, x = non_zero_indices[0], non_zero_indices[1]
pixel_values = gray_image[y, x]

# 将坐标（x, y）和像素值（pixel_values）合并成一个特征矩阵（X），其中（x, y）是两列坐标
X = np.column_stack((x, y))

# 进行DBSCAN聚类
# Epsilon（eps）和MinPts是DBSCAN的两个关键参数
# 根据你的数据和聚类要求调整它们的值
eps = 3  # 调整此值
min_pts = 3 # 调整此值
dbscan = DBSCAN(eps=eps, min_samples=min_pts)
dbscan_labels = dbscan.fit_predict(X)

# 将聚类结果转换为DataFrame便于分析
import pandas as pd
data = pd.DataFrame(X, columns=['x', 'y'])
data['label'] = dbscan_labels

# 提取核心点、边界点和噪声点
core_samples_mask = np.zeros_like(dbscan_labels, dtype=bool)
core_samples_mask[dbscan.core_sample_indices_] = True
data['is_core'] = core_samples_mask

# 输出核心点、边界点和噪声点
print("核心点数量:", len(data[data['is_core']]))
print("边界点数量:", len(data[data['label'] == -1]) - len(data[data['is_core']]))
print("噪声点数量:", len(data[data['label'] == -1]))

# 输出聚类数量（排除噪声点）
n_clusters = len(np.unique(dbscan_labels)) - 1
print("有效聚类数量（排除噪声点）:", n_clusters)

# 可视化聚类结果
plt.figure(figsize=(12, 6))

# 绘制灰度图像
plt.subplot(1, 2, 1)
plt.imshow(gray_image, cmap='gray')
plt.title("灰度图像")

# 绘制聚类结果，并翻转y轴
plt.subplot(1, 2, 2)
plt.scatter(x, y, c=dbscan_labels, cmap='viridis', marker='o', s=5)
plt.title("DBSCAN聚类结果")
plt.xlabel("X坐标")
plt.ylabel("Y坐标")
plt.gca().invert_yaxis()

plt.show()

import cv2
import numpy as np
import matplotlib.pyplot as plt
from sklearn.mixture import GaussianMixture

# 加载灰度图像
image_path = "/content/merged_image.png"
gray_image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)

# 提取非零像素点的坐标和像素值
non_zero_indices = np.nonzero(gray_image)
y, x = non_zero_indices[0], non_zero_indices[1]
pixel_values = gray_image[y, x]

# 将坐标（x, y）合并成一个特征矩阵（data）
data = np.column_stack((x, y))

# 使用GMM进行聚类
n_components = 4  # 簇的数量，根据数据形状设定
gmm = GaussianMixture(n_components=n_components, random_state=0)
gmm.fit(data)
labels = gmm.predict(data)

# 可视化聚类结果
plt.figure(figsize=(12, 6))

# 绘制灰度图像
plt.subplot(1, 2, 1)
plt.imshow(gray_image, cmap='gray')
plt.title("灰度图像")

# 绘制GMM聚类结果，并翻转y轴
plt.subplot(1, 2, 2)
plt.scatter(x, y, c=labels, cmap='viridis', marker='o', s=10)
plt.title("GMM聚类结果")
plt.xlabel("X坐标")
plt.ylabel("Y坐标")
plt.gca().invert_yaxis()

plt.show()







